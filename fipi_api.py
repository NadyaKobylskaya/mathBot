import os
import re
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import urllib3

# üîï –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ –Ω–µ–±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–∞—Ö
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# üîß –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
BASE_URL = "https://oge.fipi.ru/bank/"
START_URL = urljoin(BASE_URL, "index.php?proj=DE0E276E497AB3784C3FC4CC20248DC0")
QUESTIONS_URL = urljoin(BASE_URL, "questions.php")

OUTPUT_DIR = 'oge_tasks'
TEXTS_DIR = os.path.join(OUTPUT_DIR, 'texts')
IMAGES_DIR = os.path.join(OUTPUT_DIR, 'images')

# üóÇ –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
os.makedirs(TEXTS_DIR, exist_ok=True)
os.makedirs(IMAGES_DIR, exist_ok=True)

# üîÑ –°–µ—Å—Å–∏—è
session = requests.Session()

def save_image(img_url, task_id):
    try:
        response = session.get(img_url, verify=False)
        with open(os.path.join(IMAGES_DIR, f"{task_id}.jpg"), 'wb') as f:
            f.write(response.content)
        print(f"[+] –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {task_id}.jpg")
    except Exception as e:
        print(f"[!] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {img_url}: {e}")

def parse_task(task_html, task_id):
    text = task_html.get_text(separator="\n", strip=True)
    with open(os.path.join(TEXTS_DIR, f"{task_id}.txt"), 'w', encoding='utf-8') as f:
        f.write(text)
    print(f"[+] –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∑–∞–¥–∞–Ω–∏–µ: {task_id}.txt")

    img = task_html.find('img')
    if img and img.get('src'):
        img_url = urljoin(BASE_URL, img['src'])
        save_image(img_url, task_id)

def get_csrf_tokens():
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Referer": "https://oge.fipi.ru/"
    }
    resp = session.get(START_URL, headers=headers, verify=False)
    resp.encoding = 'windows-1251'
    html = resp.text

    # üí° –ü–æ–∏—Å–∫ —Ç–æ–∫–µ–Ω–æ–≤ —á–µ—Ä–µ–∑ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
    name_match = re.search(r"<meta\s+name=['\"]csrf-token-name['\"]\s+content=['\"]([^'\"]+)['\"]", html)
    value_match = re.search(r"<meta\s+name=['\"]csrf-token-value['\"]\s+content=['\"]([^'\"]+)['\"]", html)

    if name_match and value_match:
        return name_match.group(1), value_match.group(1)
    raise Exception("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ CSRF-—Ç–æ–∫–µ–Ω—ã")

def fetch_tasks(page_offset, csrf_name, csrf_value):
    data = {
        csrf_name: csrf_value,
        "search": "1",
        "pagesize": "10",
        "proj": "DE0E276E497AB3784C3FC4CC20248DC0",
        "qpos": str(page_offset)
    }

    headers = {
        "User-Agent": "Mozilla/5.0",
        "Referer": START_URL,
    }

    try:
        response = session.post(QUESTIONS_URL, data=data, headers=headers, verify=False)
        response.encoding = 'windows-1251'
        soup = BeautifulSoup(response.text, 'html.parser')
        tasks = soup.find_all('div', class_='prob_row')

        if not tasks:
            print(f"[!] –ó–∞–¥–∞–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ —Å–º–µ—â–µ–Ω–∏–∏ {page_offset}")
            return

        print(f"[+] –ù–∞–π–¥–µ–Ω–æ –∑–∞–¥–∞–Ω–∏–π: {len(tasks)} –Ω–∞ —Å–º–µ—â–µ–Ω–∏–∏ {page_offset}")
        for i, task in enumerate(tasks, 1):
            task_id = f"page{page_offset}_task{i}"
            parse_task(task, task_id)

    except Exception as e:
        print(f"[!] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∑–∞–¥–∞–Ω–∏–π: {e}")

def main():
    print("[.] –ü–æ–ª—É—á–µ–Ω–∏–µ CSRF-—Ç–æ–∫–µ–Ω–æ–≤...")
    csrf_name, csrf_value = get_csrf_tokens()

    for offset in range(0, 30, 10):  # üîÅ –ü—Ä–æ–≤–µ—Ä—å —Å–Ω–∞—á–∞–ª–∞ 3 —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        print(f"\n[.] –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–º–µ—â–µ–Ω–∏—è {offset}...")
        fetch_tasks(offset, csrf_name, csrf_value)
